Created the repo and cloned it using vscode

then created an env for the postgre

then we created a docker compose for the postgresql databse and pgadmin - making two services

next, 
we changee the port to 5433 since I already have pgadmin on my pc and it is routed to 5432 already.

THen I used pgadmin to create the server using the details in the .env file after loading my docker up.

Apparently, 
I need to create schemas and base tables since my database is empty. I have created the database while registering the server on pgadmin.

The goal is:
what came in → cleaned → truth → meta/monitoring.

These are the schemas. They should keep things tidy this way:

raw → unprocessed API dumps

staging → cleaned but not final

core → final analytics-ready tables

ops → logs, monitoring, etc.

It's just so we separate the different types of data logically, prevent table-name collisions cos we could have users table in staging and core, and to simplify permissions
It also allows us manage this project when it scales.

Next I created the tables for the core schema - 4 in total
- economic indicator
- air quality
- weather
- alerts

Then one table for the monitoring in the ops schema
- ingestion log

These should be able to sort out the MVP before we go for robustness later


And before I forget, I need to version my sql too so I'll dump my schema into a file and commit it.

Command:
FOr the schema dump:
docker exec -t atlas_db pg_dump -U joshua -s atlas_db > schema.sql

For the full dump i.e. schema and data:
docker exec -t atlas_db pg_dump -U joshua atlas_db > full_dump.sql

For a specific table:
docker exec -t atlas_db pg_dump -U joshua -t core.econ_daily atlas_db > econ_daily.sql

For only data without schema:
docker exec -t atlas_db pg_dump -U joshua --data-only atlas_db > data_dump.sql


---
---

Day 2
After writing the etl script for worldbank loader, I faced an issue with my normalization because I tried to insert a python dictionary into postgresql

so stupid

so I had to alter that econ table's meta column to be type JSONB so I can convert the dictionary to JSON string.

We'll see how that goes

I ran it now and sqlalchemy is asking for named params when I passed the values directly.
I'll change that.

It works now.

Only that when I run it the second time, the PKs are preventing reloading. So I'll set it to ignore duplicates.
But I'm thinkking, what if worldbank updates the data? Doesn't that mean that I'll have the old one that they must have confirmed to be wrong or obsolete?

Maybe I should set it to update instead of ignore, then I'll add an updated column to the econ table so we can see when it was updated.

I added error handling to it so if there's any issue I'll see it on my logging table




Next question: Can't I generalize the loading and ingestion process? Let's see.

And I blew it!

No user! Seems I should have called my dotenv in the utils script.

Okay, we're good now.

---
---

Day 3

I just added docstrings and comments to the etl scripts so it is more easily understandable.

Now, I'll create the air quality and weather loaders and test them with my utility script and see how they fare.

It should work though. I'm not a goat, I hop.

Damn! I referenced my utility script like I was outside the etl folder.

Didn't work this time too.
Turns out Open-Meteo doesn’t support year-long ranges in one request. I need to give shorter windows. Let's go with a month for the weather loader.
The api i used for OpenAQ needs fixing.

I'll just do a loop instead for both just for uniformity and to increase the value of this work.

